---
# This workflow builds and deploys
# the Orchestration API Docker image to Docker Hub
# and deploys it to a Kubernetes cluster using Helm.

name: Build and Deploy the Orchestration API

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - "**"
  workflow_dispatch:

jobs:
  checkout:
    name: Checkout Code
    runs-on: ubuntu-latest
    outputs:
      repo-path: ${{ steps.set-repo-path.outputs.repo-path }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set repo path output
        id: set-repo-path
        run: echo "repo-path=${GITHUB_WORKSPACE}" >> $GITHUB_OUTPUT

  validate:
    name: Validate Python, YAML file Syntax and Testcoverage
    runs-on: ubuntu-latest
    needs: checkout
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Lint Python files
        run: |
          pylint app/ --fail-under=${{ vars.PYLINT_EXPECTED_SCORE }}

      - name: Run tests with coverage
        run: pytest --cov=app --cov-fail-under=${{ vars.PYTEST_EXPECTED_PERCENTAGE }} app/tests/

      - name: Set up Helm
        uses: azure/setup-helm@v4
        with:
          version: "latest"

      - name: Lint Helm chart
        run: helm lint charts/orchestration-api

  test-db-migrations:
    name: Test Alembic DB Migrations
    runs-on: ubuntu-latest
    needs: validate
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: orchestration_db
        ports:
          - 5433:5432
        options: >-
          --health-cmd="pg_isready -U postgres"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Wait for PostgreSQL to be ready
        run: |
          for i in {1..10}; do
            pg_isready -h localhost -p 5433 -U postgres && break
            sleep 2
          done

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Alembic migrations
        env:
          DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5433/orchestration_db
        run: alembic upgrade head

  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: validate
    outputs:
      image_name: ${{ steps.set-orchestration-image.outputs.image_name }}
      image_tag: ${{ steps.set-orchestration-image.outputs.image_tag }}
      workload_timing_watcher_image_name: ${{ steps.set-workload-timing-watcher-image.outputs.workload_timing_watcher_image_name }}
      workload_timing_watcher_image_tag: ${{ steps.set-workload-timing-watcher-image.outputs.workload_timing_watcher_image_tag }}
      alerts_populator_image_name: ${{ steps.set-alerts-populator-image.outputs.alerts_populator_image_name }}
      alerts_populator_image_tag: ${{ steps.set-alerts-populator-image.outputs.alerts_populator_image_tag }}
      tuning_parameters_populator_image_name: ${{ steps.set-tuning-parameters-populator-image.outputs.tuning_parameters_populator_image_name }}
      tuning_parameters_populator_image_tag: ${{ steps.set-tuning-parameters-populator-image.outputs.tuning_parameters_populator_image_tag }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build Orchestration API Docker image
        id: set-orchestration-image
        run: |
          IMAGE_TAG=${{ vars.ORCHESTRATION_API_IMAGE_TAG }}-$(date +%Y%m%d%H%M%S)
          docker build -t ${{ vars.ORCHESTRATION_API_IMAGE_NAME }}:$IMAGE_TAG -f ./Dockerfile .
          echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "image_name=${{ vars.ORCHESTRATION_API_IMAGE_NAME }}" >> $GITHUB_OUTPUT

      - name: Push Orchestration API Docker image
        if: github.event_name == 'push'
        run: |
          docker push ${{ vars.ORCHESTRATION_API_IMAGE_NAME }}:${{ steps.set-orchestration-image.outputs.image_tag }}

      - name: Build Workload Timing Watcher Docker image
        id: set-workload-timing-watcher-image
        run: |
          IMAGE_TAG=${{ vars.WORKLOAD_TIMING_WATCHER_IMAGE_TAG }}-$(date +%Y%m%d%H%M%S)
          docker build -t ${{ vars.WORKLOAD_TIMING_WATCHER_IMAGE_NAME }}:$IMAGE_TAG -f ./service/workload-timing-watcher/Dockerfile ./service/workload-timing-watcher
          echo "workload_timing_watcher_image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "workload_timing_watcher_image_name=${{ vars.WORKLOAD_TIMING_WATCHER_IMAGE_NAME }}" >> $GITHUB_OUTPUT

      - name: Push Workload Timing Watcher Docker image
        if: github.event_name == 'push'
        run: |
          docker push ${{ vars.WORKLOAD_TIMING_WATCHER_IMAGE_NAME }}:${{ steps.set-workload-timing-watcher-image.outputs.workload_timing_watcher_image_tag }}

      - name: Build Alerts Populator Docker image
        id: set-alerts-populator-image
        run: |
          IMAGE_TAG=${{ vars.ALERTS_POPULATOR_IMAGE_TAG }}-$(date +%Y%m%d%H%M%S)
          docker build -t ${{ vars.ALERTS_POPULATOR_IMAGE_NAME }}:$IMAGE_TAG -f ./service/alerts-populator/Dockerfile .
          echo "alerts_populator_image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "alerts_populator_image_name=${{ vars.ALERTS_POPULATOR_IMAGE_NAME }}" >> $GITHUB_OUTPUT

      - name: Push Alerts Populator Docker image
        if: github.event_name == 'push'
        run: |
          docker push ${{ vars.ALERTS_POPULATOR_IMAGE_NAME }}:${{ steps.set-alerts-populator-image.outputs.alerts_populator_image_tag }}

      - name: Build Tuning Parameters Populator Docker image
        id: set-tuning-parameters-populator-image
        run: |
          IMAGE_TAG=${{ vars.TUNING_PARAMETERS_POPULATOR_IMAGE_TAG }}-$(date +%Y%m%d%H%M%S)
          docker build -t ${{ vars.TUNING_PARAMETERS_POPULATOR_IMAGE_NAME }}:$IMAGE_TAG -f ./service/tuning-parameters-populator/Dockerfile .
          echo "tuning_parameters_populator_image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "tuning_parameters_populator_image_name=${{ vars.TUNING_PARAMETERS_POPULATOR_IMAGE_NAME }}" >> $GITHUB_OUTPUT

      - name: Push Tuning Parameters Populator Docker image
        if: github.event_name == 'push'
        run: |
          docker push ${{ vars.TUNING_PARAMETERS_POPULATOR_IMAGE_NAME }}:${{ steps.set-tuning-parameters-populator-image.outputs.tuning_parameters_populator_image_tag }}

  helmPublish:
    name: Publish Helm Charts
    runs-on: ubuntu-latest
    # if: github.event_name == 'push'
    needs: validate
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history for versioning

      - name: Configure Git
        run: |
          git config user.name "$GITHUB_ACTOR"
          git config user.email "$GITHUB_ACTOR@users.noreply.github.com"

      - name: Install Helm
        uses: azure/setup-helm@v4
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Add kubernetes-dashboard dependency Helm repo
        run: helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/

      - name: Update Helm dependencies for k8s-dashboard chart
        run: helm dependency update ./charts/k8s-dashboard

      - name: Build Helm dependencies for k8s-dashboard chart
        run: helm dependency build ./charts/k8s-dashboard

      - name: Update Helm dependencies for ./charts/workload-timing-watcher chart
        run: helm dependency update ./charts/workload-timing-watcher

      - name: Build Helm dependencies for ./charts/workload-timing-watcher chart
        run: helm dependency build ./charts/workload-timing-watcher

      - name: Update Helm dependencies for orchestration-api chart
        run: helm dependency update ./charts/orchestration-api

      - name: Build Helm dependencies for orchestration-api chart
        run: helm dependency build ./charts/orchestration-api

      - name: Run chart-releaser
        uses: helm/chart-releaser-action@v1.7.0
        with:
          skip_existing: true
          packages_with_index: true
          pages_branch: gh-pages
          mark_as_latest: true
        env:
          CR_TOKEN: "${{ secrets.GITHUB_TOKEN }}"

      - name: Publish Helm charts
        uses: stefanprodan/helm-gh-pages@master
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Print Helm repo access URL
        run: |
          echo "Helm repo published at:"
          echo "https://${{ github.repository_owner }}.github.io/orchestration_library/index.yaml"
          echo ""
          echo "To use this repo:"
          echo "helm repo add orchestration_library https://${{ github.repository_owner }}.github.io/orchestration_library"
          echo "helm repo update"
          echo "helm install my-release orchestration_library/k8s-dashboard"
          echo "helm install my-release orchestration_library/pod-timing-watcher"
          echo "helm install my-release orchestration_library/orchestration-api"

  deploy:
    name: Deploy to Kubernetes
    runs-on: ubuntu-latest
    needs:
      - build-and-push
      - test-db-migrations
    strategy:
      matrix:
        kubeconfig_secret_name: [HIRO_KUBE_CONFIG_AWS, HIRO_KUBE_CONFIG_BASTON]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "latest"

      - name: Set up Helm
        uses: azure/setup-helm@v4
        with:
          version: "3.14.4"

      - name: Set up kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets[matrix.kubeconfig_secret_name] }}" > ~/.kube/${{ matrix.kubeconfig_secret_name }}
          chmod 600 ~/.kube/${{ matrix.kubeconfig_secret_name }}

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Add kubernetes-dashboard Helm repo
        run: helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/

      - name: Update Helm dependencies for k8s-dashboard chart
        run: helm dependency build ./charts/k8s-dashboard

      - name: Update Helm dependencies for workload-timing-watcher chart
        run: helm dependency build ./charts/workload-timing-watcher

      - name: Update Helm dependencies for alerts-populator chart
        run: helm dependency build ./charts/alerts-populator

      - name: Update Helm dependencies for tuning-parameters-populator chart
        run: helm dependency build ./charts/tuning-parameters-populator

      - name: Update Helm dependencies for orchestration-api chart
        run: helm dependency build ./charts/orchestration-api

      - name: Set Helm dry-run flag
        id: helm-dry-run
        run: |
          if [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]] || [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "dry_run_flag=" >> $GITHUB_OUTPUT
          else
            echo "dry_run_flag=--dry-run" >> $GITHUB_OUTPUT
          fi

      # - name: Deploy the Kubernetes Dashboard with reverse proxy to the cluster
      #   # Ignore CSRF secret template errors in dry-run/template mode
      #   # (see https://github.com/kubernetes/dashboard/issues/7908)
      #   # This step will always pass, even if the CSRF error occurs, due to '|| true'
      #   run: |
      #     helm upgrade --install ${{ vars.KUBERNETES_DASHBOARD_RELEASE_NAME}} ./charts/k8s-dashboard \
      #       --namespace ${{ vars.KUBERNETES_DASHBOARD_NAMESPACE }} \
      #       --create-namespace \
      #       --set serviceAccountName=${{ vars.KUBERNETES_DASHBOARD_RO_SA }} \
      #       --set reverseProxy.name=aces-dashboard-reverse-proxy \
      #       --set reverseProxy.service.port=80 \
      #       --set reverseProxy.service.type=${{ vars.KUBERNETES_DASHBOARD_SERVICE_TYPE }} \
      #       --set reverseProxy.service.nodePort=${{ vars.KUBERNETES_DASHBOARD_NODE_PORT }} \
      #       ${{ steps.helm-dry-run.outputs.dry_run_flag }}

      - name: Set Helm Cert-Manager CRD install flag
        id: helm-install-cert-manager
        run: |
          # List of CRDs you expect from k8s-dashboard or its dependencies
          if kubectl get crd certificaterequests.cert-manager.io --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} >/dev/null; then
            echo "cert-man-crd-install=false" >> $GITHUB_OUTPUT
          else
            echo "cert-man-crd-install=true" >> $GITHUB_OUTPUT
          fi

      - name: Set Helm values for Service type and Ingress
        id: helm-app-access
        run: |
          if [[ "${{ matrix.kubeconfig_secret_name }}" == "HIRO_KUBE_CONFIG_AWS" ]]; then
            echo "orchestration_api_service_type=--set app.service.type=NodePort" >> $GITHUB_OUTPUT
            echo "k8s_dashboard_service_type=--set k8sDashboard.reverseProxy.service.type=NodePort" >> $GITHUB_OUTPUT
            echo "orchestration_api_service_nodePort=--set app.service.nodePort=${{ vars.ORCHESTRATION_API_NODE_PORT }}" >> $GITHUB_OUTPUT
            echo "k8s_dashboard_service_nodePort=--set k8sDashboard.reverseProxy.service.nodePort=${{ vars.KUBERNETES_DASHBOARD_NODE_PORT }}" >> $GITHUB_OUTPUT
            echo "orchestration_api_ingress_enabled=--set app.ingress.enabled=false" >> $GITHUB_OUTPUT
            echo "k8s_dashboard_ingress_enabled=--set k8sDashboard.reverseProxy.ingress.enabled=false" >> $GITHUB_OUTPUT            
            echo "orchestration_api_ingress_host=" >> $GITHUB_OUTPUT
            echo "k8s_dashboard_ingress_host=" >> $GITHUB_OUTPUT
          else
            echo "orchestration_api_service_type=--set app.service.type=ClusterIP" >> $GITHUB_OUTPUT
            echo "k8s_dashboard_service_type=--set k8sDashboard.reverseProxy.service.type=ClusterIP" >> $GITHUB_OUTPUT
            echo "orchestration_api_service_nodePort=" >> $GITHUB_OUTPUT
            echo "k8s_dashboard_service_nodePort=" >> $GITHUB_OUTPUT
            echo "orchestration_api_ingress_enabled=--set app.ingress.enabled=true" >> $GITHUB_OUTPUT
            echo "k8s_dashboard_ingress_enabled=--set k8sDashboard.reverseProxy.ingress.enabled=true" >> $GITHUB_OUTPUT
            echo "orchestration_api_ingress_host=--set app.ingress.host=${{ vars.ORCHESTRATION_API_INGRESS_HOST }}" >> $GITHUB_OUTPUT
            echo "k8s_dashboard_ingress_host=--set k8sDashboard.reverseProxy.ingress.host=${{ vars.KUBERNETES_DASHBOARD_INGRESS_HOST }}" >> $GITHUB_OUTPUT
          fi

      - name: Set Dashboard URL
        id: set-dashboard-url
        run: |
          if [[ -n "${{ secrets.KUBERNETES_DASHBOARD_URL }}" ]]; then
            DASHBOARD_URL="${{ secrets.KUBERNETES_DASHBOARD_URL }}"
          elif [[ "${{ steps.helm-app-access.outputs.k8s_dashboard_service_type }}" == "--set k8sDashboard.reverseProxy.service.type=NodePort" ]]; then
            DASHBOARD_URL="http://${{ secrets.KUBERNETES_DASHBOARD_NODE_IP }}:${{ vars.KUBERNETES_DASHBOARD_NODE_PORT }}/"
          elif [[ "${{ steps.helm-app-access.outputs.k8s_dashboard_ingress_enabled }}" == "--set k8sDashboard.reverseProxy.ingress.enabled=true" ]]; then
            DASHBOARD_URL="https://${{ vars.KUBERNETES_DASHBOARD_INGRESS_HOST }}/"
          else
            DASHBOARD_URL=""
          fi
          echo "dashboard_url=$DASHBOARD_URL" >> $GITHUB_OUTPUT

      - name: Prepare --set arguments for NATS_JS_SUBJECTS array of Alerts Populator Service
        id: set-alerts-nats-js-subjects
        run: |
          alerts_nats_js_subjects_set=""
          IFS=',' read -ra SUBJECTS <<< "${{ vars.ALERTS_POPULATOR_NATS_JS_SUBJECTS }}"
          for i in "${!SUBJECTS[@]}"; do
          alerts_nats_js_subjects_set+="--set alertsPopulator.env.NATS_JS_SUBJECTS[$i]=${SUBJECTS[$i]} "
          done
          alerts_nats_js_subjects_set=${alerts_nats_js_subjects_set% }
          echo "alerts_nats_js_subjects_set=$alerts_nats_js_subjects_set" >> $GITHUB_OUTPUT

      - name: Prepare --set arguments for NATS_JS_SUBJECTS array of Tuning Parameters Populator Service
        id: set-tuning-nats-js-subjects
        run: |
          tuning_nats_js_subjects_set=""
          IFS=',' read -ra SUBJECTS <<< "${{ vars.TUNING_PARAMETERS_POPULATOR_NATS_JS_SUBJECTS }}"
          for i in "${!SUBJECTS[@]}"; do
          tuning_nats_js_subjects_set+="--set tuningParametersPopulator.env.NATS_JS_SUBJECTS[$i]=${SUBJECTS[$i]} "
          done
          tuning_nats_js_subjects_set=${tuning_nats_js_subjects_set% }
          echo "tuning_nats_js_subjects=$tuning_nats_js_subjects_set" >> $GITHUB_OUTPUT

      - name: Ensure post-renderer script executable
        run: |
          ls -l ./charts/orchestration-api/add-common-labels.sh || exit 1
          chmod +x ./charts/orchestration-api/add-common-labels.sh
          yq --version || echo "yq not installed"

      # - name: Install shc compiler
      #   run: |
      #     sudo apt-get update
      #     sudo apt-get install -y shc

      # - name: Compile post-renderer Bash script
      #   id: compile-post-renderer
      #   run: |
      #     echo "Compiling add-common-labels.sh to binary..."
      #     shc -f ./charts/orchestration-api/add-common-labels.sh \
      #         -x /usr/bin/bash \
      #         -o ./charts/orchestration-api/add-common-labels-bin
          
      #     chmod +x ./charts/orchestration-api/add-common-labels-bin
          
      #     echo "Compiled binary:"
      #     ls -l ./charts/orchestration-api/add-common-labels-bin

      #     echo "add_common_labels_bin=./charts/orchestration-api/add-common-labels-bin" >> $GITHUB_OUTPUT

      # - name: Test binary with Helm template
      #   run: |
      #     helm template ./charts/orchestration-api \
      #       | ./charts/orchestration-api/add-common-labels-bin \
      #       > /dev/null
      #     echo "Binary post-renderer executed successfully."

      - name: Deploy Orchestration API Helm chart with its dependencies
        env:
          RELEASE_NAME: ${{ vars.ORCHRESTRATION_API_RELEASE_NAME }} # used in post-renderer script
        run: |
          helm upgrade --install ${{ vars.ORCHRESTRATION_API_RELEASE_NAME }} ./charts/orchestration-api \
            --post-renderer "./charts/orchestration-api/add-common-labels.sh" \
            --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} \
            --namespace ${{ vars.ORCHESTRATION_API_NAMESPACE }} \
            --create-namespace \
            --set app.name=${{ vars.ORCHRESTRATION_API_RELEASE_NAME }} \
            --set app.image.repository=${{ vars.ORCHESTRATION_API_IMAGE_NAME }} \
            --set app.image.tag=${{ needs.build-and-push.outputs.image_tag }} \
            ${{ steps.helm-app-access.outputs.orchestration_api_service_type }} \
            ${{ steps.helm-app-access.outputs.orchestration_api_service_nodePort }} \
            ${{ steps.helm-app-access.outputs.orchestration_api_ingress_enabled }} \
            ${{ steps.helm-app-access.outputs.orchestration_api_ingress_host }} \
            --set app.env.ALERT_CRITICAL_THRESHOLD=${{ vars.ALERT_CRITICAL_THRESHOLD }} \
            --set app.env.ALERT_CRITICAL_THRESHOLD_WINDOW_SECONDS=${{ vars.ALERT_CRITICAL_THRESHOLD_WINDOW_SECONDS }} \
            --set app.env.ALERT_ACTION_TRIGGER_SERVICE_URL=${{ vars.ALERT_ACTION_TRIGGER_SERVICE_URL }} \
            --set app.env.NATS_SERVER=${{ vars.ORCHESTRATION_API_NATS_SERVER }} \
            --set app.env.NATS_KPI_JS_STREAM=${{ vars.ORCHESTRATION_API_NATS_KPI_JS_STREAM }} \
            --set app.env.NATS_KPI_JS_SUBJECT=${{ vars.ORCHESTRATION_API_NATS_KPI_JS_SUBJECT }} \
            --set k8sDashboard.enabled=true \
            --set k8sDashboard.serviceAccountName=${{ vars.KUBERNETES_DASHBOARD_RO_SA }} \
            --set k8sDashboard.accessURL=${{ steps.set-dashboard-url.outputs.dashboard_url }} \
            ${{ steps.helm-app-access.outputs.k8s_dashboard_service_type }} \
            ${{ steps.helm-app-access.outputs.k8s_dashboard_service_nodePort }} \
            ${{ steps.helm-app-access.outputs.k8s_dashboard_ingress_enabled }} \
            ${{ steps.helm-app-access.outputs.k8s_dashboard_ingress_host }} \
            --set k8sDashboard.dashboard.cert-manager.installCRDs=${{ steps.helm-install-cert-manager.outputs.cert-man-crd-install }} \
            --set runMigration=true \
            --set global.postgresql.auth.postgresPassword=postgress \
            --set global.postgresql.auth.password=postgress \
            --set workloadTimingWatcher.image.repository=${{ vars.WORKLOAD_TIMING_WATCHER_IMAGE_NAME }} \
            --set workloadTimingWatcher.image.tag=${{ needs.build-and-push.outputs.workload_timing_watcher_image_tag }} \
            --set alertsPopulator.image.repository=${{ vars.ALERTS_POPULATOR_IMAGE_NAME }} \
            --set alertsPopulator.image.tag=${{ needs.build-and-push.outputs.alerts_populator_image_tag }} \
            --set alertsPopulator.env.NATS_SERVER=${{ vars.ALERTS_POPULATOR_NATS_SERVER }} \
            --set alertsPopulator.env.NATS_JS_STREAM=${{ vars.ALERTS_POPULATOR_NATS_JS_STREAM }} \
            --set alertsPopulator.env.NATS_JS_DURABLE=${{ vars.ALERTS_POPULATOR_NATS_JS_DURABLE }} \
            ${{ steps.set-alerts-nats-js-subjects.outputs.alerts_nats_js_subjects_set }} \
            --set tuningParametersPopulator.image.repository=${{ vars.TUNING_PARAMETERS_POPULATOR_IMAGE_NAME }} \
            --set tuningParametersPopulator.image.tag=${{ needs.build-and-push.outputs.tuning_parameters_populator_image_tag }} \
            --set tuningParametersPopulator.env.NATS_SERVER=${{ vars.TUNING_PARAMETERS_POPULATOR_NATS_SERVER }} \
            --set tuningParametersPopulator.env.NATS_JS_STREAM=${{ vars.TUNING_PARAMETERS_POPULATOR_NATS_JS_STREAM }} \
            --set tuningParametersPopulator.env.NATS_JS_DURABLE=${{ vars.TUNING_PARAMETERS_POPULATOR_NATS_JS_DURABLE }} \
            ${{ steps.set-tuning-nats-js-subjects.outputs.tuning_nats_js_subjects }} \
            --timeout=600s \
            --wait \
            --debug \
            ${{ steps.helm-dry-run.outputs.dry_run_flag }}

      - name: Wait for Alembic migration job and show logs
        if: github.event_name == 'push'
        run: |
          JOB_NAME=$(kubectl get jobs -n ${{ vars.ORCHESTRATION_API_NAMESPACE }} --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} -o json \
            | jq -r '.items | map(select(.metadata.name | startswith("alembic-migration-"))) | sort_by(.metadata.creationTimestamp) | last(.[]).metadata.name')

          echo "Waiting for Job: $JOB_NAME"
          kubectl wait --for=condition=complete --timeout=600s job/$JOB_NAME -n ${{ vars.ORCHESTRATION_API_NAMESPACE }} --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} || JOB_STATUS=$?
          JOB_STATUS=$?

          echo "Job status:"
          kubectl describe job $JOB_NAME -n ${{ vars.ORCHESTRATION_API_NAMESPACE }} --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} || true
          echo "Pods for job:"
          kubectl get pods -n ${{ vars.ORCHESTRATION_API_NAMESPACE }} -l job-name=$JOB_NAME -o wide --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} || true

          POD_NAME=$(kubectl get pods -n ${{ vars.ORCHESTRATION_API_NAMESPACE }} -l job-name=$JOB_NAME -o json --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} | jq -r '.items[0].metadata.name // empty')
          if [ -z "$POD_NAME" ]; then
            echo "No pods found for Alembic migration job. Check job status above."
            exit 1
          fi

          # Wait for pod to be in a loggable state (Running, Succeeded, or Failed)
          for i in {1..10}; do
            PHASE=$(kubectl get pod -n ${{ vars.ORCHESTRATION_API_NAMESPACE }} $POD_NAME --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} -o jsonpath="{.status.phase}")
            if [[ "$PHASE" == "Running" || "$PHASE" == "Succeeded" || "$PHASE" == "Failed" ]]; then
              break
            fi
            echo "Pod $POD_NAME is in $PHASE state, waiting for it to be loggable..."
            sleep 5
          done

          echo "Pod logs:"
          kubectl logs -n ${{ vars.ORCHESTRATION_API_NAMESPACE }} $POD_NAME --kubeconfig ~/.kube/${{ matrix.kubeconfig_secret_name }} || true

          # Fail the step if the job did not succeed
          if [ $JOB_STATUS -ne 0 ]; then
            echo "Alembic migration job failed or timed out."
            exit 1
          fi
